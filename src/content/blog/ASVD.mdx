---
heroImage: /images/Screenshot 2024-12-14 211605.png
category: 学习
description: >-
  论文《ASVD: Activation-aware Singular Value Decomposition for Compressing Large
  Language Models》的阅读记录
pubDate: 2024-12-13T16:00:00.000Z
draft: false
tags:
  - decomposition
  - low-rank
  - llm
  - kv cache
  - compression
  - svd
title: 论文《ASVD》的阅读记录
---

# ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models

链接：[https://arxiv.org/abs/2312.05821](https://arxiv.org/abs/2312.05821)

## 一、发现的问题

1. 激活中的异常值可能会加剧分解误差。这些异常值可能会不成比例地影响矩阵近似的准确性，导致压缩结果不理想。
2. 某些层对分解的敏感性高于其他层，统一分解这些层可能会导致性能显著下降

## 二、解决的方法

1. 激活值感知的SVD分解方法（ASVD: Activation-aware Singular Value Decomposition），即在分解权重矩阵W的过程中考虑激活值的分布。
   1. 在分解权重矩阵的过程中考虑激活值的分布
   2. 通过一个缩放矩阵S对权重矩阵的列进行变换。该缩放矩阵是基于跨输入激活通道的分布模式设计的。这种调整对于有异常值的激活尤为有益，使分解能够对这些特定的权重给予更多的关注
2. 基于敏感度的截断秩搜索机制。
   1. 多头注意力层中的权重相比于多层感知器层中的权重，对分解的抗性更强。
   2. ASVD评估每个层在不同秩下的分解敏感性，使能够为最优分解分配合适的秩。

## 三、其他细节

1. **应用ASVD的KV缓存压缩**
   ![](</images/屏幕截图 2024-12-19 201416.png> "ASVD如何降低K缓存的内存成本的演示")
   （左）随着文本长度L的增加，K缓存所需的存储空间也随着线性增加。（右）ASVD将K的投影权重矩阵W分解为两个低秩矩阵U和V。这种低秩结构允许K表示存储在降维后的r维空间中。因此，我们只需要在r维而不是N维空间中保存K，节省了N/r倍的内存。V同理。
2. **通过将奇异值融入左右矩阵降低量化误差**

确保矩阵更均匀的分布，从而减少了不同通道之间的差异并降低量化误差

## 四、问题

**1.变化矩阵S是怎么样做到减轻分解误差的？**

**2.根据激活值来分解权重矩阵如何做到泛化的效果？**

激活值X是从一个小的校准集中读取的。而该校准集是从预训练的数据集中提取的，以避免对特定任务过拟合。
